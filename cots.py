from langchain_community.document_loaders import PyPDFLoader
from langchain.vectorstores.chroma import Chroma
# from langchain_community.vectorstores import Chroma
# from langchain_chroma import Chroma

from langchain_text_splitters import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
)

from langchain.schema import HumanMessage, SystemMessage


from typing import List

import time

# from langchain_community.embeddings import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

import os, shutil

# from chroma_loader import save_to_chroma


os.environ['OPENAI_API_KEY'] = ""



filepath = ("pdf_documents/Indian Polity - 5th Edition - M Laxmikanth.pdf")

chunk_size = 1000
chunk_overlap = chunk_size//10

CHROMA_PATH = "chroma"




model_name = "gpt-3.5-turbo"


def query_openai_list(system_message, user_message, model_name= "gpt-3.5-turbo"):
    model = ChatOpenAI(
        model_name=model_name,
        temperature=0,
        top_p=0
    )

    message = [
        SystemMessage(
            content=system_message
        ),
        HumanMessage(
            content=user_message
        ),
    ]

    list_model = model.with_structured_output(list)

    response = list_model.invoke(message)

    print(response)

    return response['iterable']


def query_openai_final_response(system_message, user_message, model_name= "gpt-3.5-turbo"):
    model = ChatOpenAI(
        model_name=model_name,
        temperature=0,
        top_p=0
    )

    message = [
        SystemMessage(
            content=system_message
        ),
        HumanMessage(
            content=user_message
        ),
    ]

    # list_model = model.with_structured_output(list)

    response = model.invoke(message)

    print(response)

    return response.content

#A prompt will be given as input. Break that prompt into 3 small prompts that each small prompt generate a result. Utimately this would be helping in generating better soution in next stage.

PROBLEM_BREAKDOWN_MESSAGE = """
You are Indian Polity Expert. A query needs to break down upto 3 sub problems not related to each other but w.r.t the context. Utimately this would be helping in generating better soution in next stage.

Examples:

What is the difference between “vote-on-account” and “interim budget”?

can be broken down as:
- what is vote-on-account?
- what is interim budget?
- what is the difference between them?

Generate the output from the retrieved documents itself.
If you don't know the soultion, just give output as "Don't Know about it."

"""



# def user_query(mess):

query = "In India, if a religious sect/community is given “the status of a national minority, what special advantages it is entitled to?"

user_message = f"question to be broken down:  {query}"



response = query_openai_list(PROBLEM_BREAKDOWN_MESSAGE, user_message)

print(response)
print(type(response))




"""
iterating over each question
generate answer
use question, answer an context to the next sub problem
"""
embedding_function = OpenAIEmbeddings()
db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

# Retrieving the context from the DB using similarity search


# Check if there are any matching results or if the relevance score is too low
# if len(results) == 0 or results[0][1] < 0.7:
#     print(f"Unable to find matching results.")


COT_BRAINSTORM_SIMPLE_PROBLEM_SYSTEM_MESSAGE = """You are an AI that is part of a larger brainstorming group, tasked with finding a solution to a bigger problem.
This bigger problem has been broken down, and each of these smaller problems will be given to a different AI, including you.
Your task is to simulate 3 experts that will be given this smaller problem plus a small collection of snippets extracted from technical documents, and will work together a solution.
You are supposed to deduce your observations and solutions ONLY out of the provided snippets. If the snippets don't provide enough information, don't assume or make stuff up.
You will also receive a summarized version of the other smaller problems and the solutions proposed to them. Take that information into account when brainstorming the problem you were given to resolve.
The flow is: expert 1 will generate an observation about the problem. Then expert 2 will take expert 1's observation plus what has been discussed so far, and generate a new observation. This observation generated by expert 2 will then be used by expert 3 to generate his own observation. At the end, I want you to take all this discussion and generate a final solution to the problem."""

COT_GENERATE_BIG_SOLUTION_SYSTEM_MESSAGE = f"""You are an Indian Polity (UPSC) expert that will be given a problem, and a list of observations and potential solutions to this problem. The problem may have different requirements, and each solution in the list is accounting for one of these requirements.
It's your job to take all these solutions together, and come up with a final solution that should meet all the requirements of the original problem"""



def brainstorm_simple_problem(simple_problem: str, snippets: list[str], other_problems_and_solutions: list[str]):
        """
        Search for a solution to a problem, by simulating 3 experts that together brainstorm the solution. Relevant snippets
        and the other small problems and solutions are also provided to the LLM, in order to help give it the full context.
        """

        system_message = COT_BRAINSTORM_SIMPLE_PROBLEM_SYSTEM_MESSAGE

        snippets_text = str()
        for idx, snippet in enumerate(snippets):
            snippet_text = f"<SNIPPET_{idx + 1}\n\n{snippet}\n\n<END_SNIPPET_{idx + 1}"
            snippets_text += snippet_text

        joined_other_problems_and_solutions = '\n\n'.join(other_problems_and_solutions)
        prompt = f"""The problem is: {simple_problem}
The snippets to use are:    
{snippets_text}

The other smaller problems and their solutions are:

{joined_other_problems_and_solutions if joined_other_problems_and_solutions else ""}
"""

        response = query_openai_final_response(system_message, prompt)
        system_message = f"""You will be given observations made by 3 experts that were working together to resolve a problem. Your task is to summarize their findings in just one concise response."""
        summarized_response = query_openai_final_response(system_message, response)

        return summarized_response



problems_so_far = []

all_chunks = []

solutions = []

for problem in response:
    chunks = db.similarity_search_with_relevance_scores(problem, k=3)
    print(chunks)
    # break
    for chunk in chunks:
        if chunk not in all_chunks:
            all_chunks.append(chunk)
    
    sub_solution = brainstorm_simple_problem(problem, all_chunks, problems_so_far)
    problem_and_solution = f"Problem: {problem}\n\nProposed solution: {sub_solution}"
    problems_so_far.append(problem_and_solution)
    solutions.append(sub_solution)



def summarize_solution() -> str:
        """
        Given a list of solutions for the small problems, and the original "big" problem provided by the user, generate
        a summarized solution
        """

        system_message = COT_GENERATE_BIG_SOLUTION_SYSTEM_MESSAGE

        joined_solutions = '\n\n'.join(solutions)

        prompt = f"""Problem: {query}
Solutions: {joined_solutions}

Summarized solution:"""

        response = query_openai_final_response(system_message, prompt)

        return response#.content


print(summarize_solution.__dict__)
